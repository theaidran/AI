{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theaidran/make_me_summary/blob/main/make_me_summary_ui_4bit_textgen_gdrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFQl6-FjSYtY"
      },
      "source": [
        "# Make me summary text generation for Google Colab\n",
        "\n",
        "This notebook uses [https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui)\n",
        "\n",
        "‚ñ∂‚è©Run all the cells and a URL will appear at the bottom in around 5 minutes.ü§ûüê±‚Äçüë§\n",
        "\n",
        "Then, choose the .txt file to be summarized and click the \"Make summary\" button.\n",
        "Summarization will be automatically saved to your browser downloads directory.\n",
        "\n",
        "\n",
        "## Parameters\n",
        "\n",
        "* **save_logs_to_google_drive**: saves your chat logs, characters, and softprompts to Google Drive automatically, so that they will persist across sessions.\n",
        "* **text_streaming**: streams the text output in real time instead of waiting for the full response to be completed.\n",
        "* **load_in_8bit**: loads the model with 8-bit precision, reducing the GPU memory usage by half. This allows you to use the full 2048 prompt length without running out of memory, at a small accuracy and speed cost.\n",
        "* **chat_language**: if different than English, activates automatic translation using Google Translate, allowing you to communicate with the bot in a different language.\n",
        "\n",
        "## Updates\n",
        "\n",
        "* check [README](https://github.com/theaidran/AI/blob/main/README.md) on github for Updates\n",
        "\n",
        "## Credits\n",
        "\n",
        "Based on the [original notebook by 81300](https://colab.research.google.com/github/81300/AI-Notebooks/blob/main/Colab-TextGen-GPU.ipynb).\n",
        "\n",
        "Forked from [Philio](https://github.com/pcrii/Philo-Colab-Collection/blob/main/4bit_TextGen_Gdrive.ipynb).\n",
        "\n",
        "Forked from [eucdee](https://github.com/eucdee/AI/blob/main/4bit_TextGen_Gdrive.ipynb).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "f7TVVj_z4flw",
        "outputId": "338d8e69-66c7-4c4d-945e-1d842101b1ae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LGQ8BiMuXMDG",
        "outputId": "27cffad9-f633-49d7-9c85-e499ffd03127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished\n",
            "Available Models\n",
            "['config.yaml', 'place-your-models-here.txt']\n"
          ]
        }
      ],
      "source": [
        "#@title 2. Install the web UI\n",
        "#remember gradio is currently held back\n",
        "save_logs_to_google_drive = False #@param {type:\"boolean\"}\n",
        "save_everything_to_google_drive = False #@param {type:\"boolean\"}\n",
        "#@markdown remember these models are large and free Gdrive is only 15Ggb <br>\n",
        "install_gptq = True #@param {type:\"boolean\"}\n",
        "#@markdown Install GPTQ-for-LLaMa for 4bit quantized models requiring --wbits 4\n",
        "from IPython.display import clear_output\n",
        "if save_logs_to_google_drive:\n",
        "  import os\n",
        "  import shutil\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  base_folder = '/content/drive/MyDrive'\n",
        "\n",
        "if save_everything_to_google_drive:\n",
        "    import os\n",
        "    import shutil\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_folder = '/content/drive/MyDrive'\n",
        "    repo_dir = '/content/drive/MyDrive/text-generation-webui'\n",
        "    model_dir = '/content/drive/MyDrive/text-generation-webui/models'\n",
        "    gptq_dir = '/content/drive/MyDrive/text-generation-webui/repositories/GPTQ-for-LLaMa'\n",
        "    if os.path.exists(repo_dir):\n",
        "        %cd {repo_dir}\n",
        "        !git pull\n",
        "    else:\n",
        "        %cd /content/drive/MyDrive/\n",
        "        !git clone https://github.com/theaidran/text-generation-webui\n",
        "\n",
        "else:\n",
        "    model_dir = '/content/text-generation-webui/models'\n",
        "    repo_dir = '/content/text-generation-webui'\n",
        "    %cd /content\n",
        "    !git clone https://github.com/theaidran/text-generation-webui\n",
        "\n",
        "\n",
        "\n",
        "if save_logs_to_google_drive:\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data\"):\n",
        "    os.mkdir(f\"{base_folder}/oobabooga-data\")\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data/logs\"):\n",
        "    os.mkdir(f\"{base_folder}/oobabooga-data/logs\")\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data/softprompts\"):\n",
        "    os.mkdir(f\"{base_folder}/oobabooga-data/softprompts\")\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data/characters\"):\n",
        "    shutil.move(\"text-generation-webui/characters\", f\"{base_folder}/oobabooga-data/characters\")\n",
        "  else:\n",
        "    !rm -r \"text-generation-webui/characters\"\n",
        "\n",
        "  !rm -r \"text-generation-webui/softprompts\"\n",
        "  !ln -s \"$base_folder/oobabooga-data/logs\" \"text-generation-webui/logs\"\n",
        "  !ln -s \"$base_folder/oobabooga-data/softprompts\" \"text-generation-webui/softprompts\"\n",
        "  !ln -s \"$base_folder/oobabooga-data/characters\" \"text-generation-webui/characters\"\n",
        "\n",
        "else:\n",
        "  !mkdir text-generation-webui/logs\n",
        "\n",
        "!ln -s text-generation-webui/logs .\n",
        "!ln -s text-generation-webui/characters .\n",
        "!ln -s text-generation-webui/models .\n",
        "%rm -r sample_data\n",
        "%cd text-generation-webui\n",
        "!wget https://raw.githubusercontent.com/pcrii/Philo-Colab-Collection/main/settings-colab-template.json -O settings-colab-template.json\n",
        "\n",
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "!pip install -r extensions/google_translate/requirements.txt\n",
        "!pip install -r extensions/silero_tts/requirements.txt\n",
        "print(f\"\\033[1;32;1m\\n --> If you see a warning about \\\"pydevd_plugins\\\", just ignore it and move on to Step 3. There is no need to restart the runtime.\\n\\033[0;37;0m\")\n",
        "\n",
        "if install_gptq:\n",
        "    if save_everything_to_google_drive:\n",
        "        if os.path.exists(gptq_dir):\n",
        "            %cd {gptq_dir}\n",
        "            !git pull\n",
        "            !pip install ninja\n",
        "            !pip install -r requirements.txt\n",
        "            !python setup_cuda.py install\n",
        "\n",
        "        else:\n",
        "            !mkdir /content/drive/MyDrive/text-generation-webui/repositories\n",
        "            %cd /content/drive/MyDrive/text-generation-webui/repositories\n",
        "            !git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa -b cuda\n",
        "            !ln -s GPTQ-for-LLaMa text-generation-webui/repositories/GPTQ-for-LLaMa\n",
        "            %cd GPTQ-for-LLaMa\n",
        "            !pip install ninja\n",
        "            !pip install -r requirements.txt\n",
        "            !pip install --upgrade transformers==4.30.0\n",
        "            !python setup_cuda.py install\n",
        "    else:\n",
        "        %mkdir /content/text-generation-webui/repositories/\n",
        "        %cd /content/text-generation-webui/repositories/\n",
        "        !git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa -b cuda\n",
        "        !mkdir -p text-generation-webui/repositories\n",
        "        !ln -s GPTQ-for-LLaMa text-generation-webui/repositories/GPTQ-for-LLaMa\n",
        "        %cd GPTQ-for-LLaMa\n",
        "        !pip install ninja\n",
        "        !pip install -r requirements.txt\n",
        "        !pip install --upgrade transformers==4.30.0\n",
        "        !python setup_cuda.py install\n",
        "clear_output()\n",
        "print(\"Finished\")\n",
        "if save_logs_to_google_drive or save_everything_to_google_drive:\n",
        "    drive_NOT_mounted = False\n",
        "else:\n",
        "    drive_NOT_mounted = True\n",
        "\n",
        "if drive_NOT_mounted:\n",
        "  import os\n",
        "print(\"Available Models\")\n",
        "print(os.listdir(model_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7t4QfXf4U9p",
        "outputId": "9888ff48-9ac1-4161-baa4-e59f58be1b84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/text-generation-webui\n",
            "Downloading the model to models/TheBloke_Vicuna-13B-1-3-SuperHOT-8K-GPTQ\n",
            "100% 11.7k/11.7k [00:00<00:00, 62.6MiB/s]\n",
            "100% 866/866 [00:00<00:00, 6.01MiB/s]\n",
            "100% 132/132 [00:00<00:00, 1.22MiB/s]\n",
            "100% 2.59k/2.59k [00:00<00:00, 20.3MiB/s]\n",
            "100% 39.5k/39.5k [00:00<00:00, 232kiB/s]\n",
            "100% 135/135 [00:00<00:00, 828kiB/s]\n",
            "100% 435/435 [00:00<00:00, 3.34MiB/s]\n",
            "100% 1.84M/1.84M [00:00<00:00, 2.14MiB/s]\n",
            "100% 500k/500k [00:00<00:00, 729kiB/s]\n",
            "100% 727/727 [00:00<00:00, 5.58MiB/s]\n",
            "100% 7.45G/7.45G [05:29<00:00, 22.6MiB/s]\n",
            "Available Models\n",
            "['config.yaml', 'TheBloke_Vicuna-13B-1-3-SuperHOT-8K-GPTQ']\n"
          ]
        }
      ],
      "source": [
        "#@title 3. Download Model\n",
        "#@markdown you can insert any huggingface model in Organization/model format\n",
        "model_download = \"TheBloke/Vicuna-13B-1-3-SuperHOT-8K-GPTQ\" #@param [ \"TheBloke/vicuna-13b-v1.3-GPTQ\", \"TheBloke/vicuna-33B-preview-GPTQ\", \"TheBloke/Vicuna-33B-1.3SuperHOT-8K-GPTQ\", \"TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g --branch actorder\", \"TheBloke/Vicuna-13B-1-3-SuperHOT-8K-GPTQ\", \"4bit/vicuna-13B-1.1-GPTQ-4bit-128g\", \"Aitrepreneur/wizardLM-7B-GPTQ-4bit-128g\", \"TheBloke/wizardLM-7B-GPTQ\", \"gozfarb/oasst-llama13b-4bit-128g\", \"catalpa/codecapybara-4bit-128g-gptq\", \"mzedp/dolly-v2-12b-GPTQ-4bit-128g\", \"autobots/pythia-12b-gptqv2-4bit\", \"TheBloke/Vicuna-13B-CoT-GPTQ\", \"TheBloke/gpt4-alpaca-lora-13B-GPTQ-4bit-128g\"] {allow-input: true}\n",
        "#@markdown remember these models are large and free Gdrive is only 15Ggb <br>\n",
        "\n",
        "%cd {repo_dir}\n",
        "!python download-model.py {model_download}\n",
        "#this lists directorys from your model folder you can copy the name provided for the model you want for use in the the next cell\n",
        "!rm {model_dir}/place-your-models-here.txt\n",
        "#clear_output()\n",
        "if save_logs_to_google_drive or save_everything_to_google_drive:\n",
        "    drive_NOT_mounted = False\n",
        "else:\n",
        "    drive_NOT_mounted = True\n",
        "\n",
        "if drive_NOT_mounted:\n",
        "  import os\n",
        "print(\"Available Models\")\n",
        "print(os.listdir(model_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hKuocueuXnm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9972e2ab-76d1-4f79-f6b9-bae2d45ab35a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tmux: server killed (pid 5834)\n",
            "python3 killed (pid 5835)\n",
            "2\n",
            "/content/text-generation-webui\n",
            "tmux new -d python server.py --api  --loader exllama --max_seq_len 2048 --compress_pos_emb 1 --model TheBloke_Vicuna-13B-1-3-SuperHOT-8K-GPTQ  --model_type LLaMa --settings settings-colab.json --extensions gallery --chat --wbits 4 \n"
          ]
        }
      ],
      "source": [
        "#@title 4. Launch\n",
        "import json\n",
        "\n",
        "#Close server if is running\n",
        "!pkill -f -e -c server.py\n",
        "!fuser -k 5000/tcp  # close api port\n",
        "!fuser -k 5005/tcp  # close stream port\n",
        "\n",
        "#@markdown if you dont know what to enter the previous cell should have printed available inputs <br> paste it here\n",
        "model_load = \"TheBloke_Vicuna-13B-1-3-SuperHOT-8K-GPTQ\" #@param {type:\"string\"}\n",
        "# Parameters\n",
        "#auto_devices = False #@param {type:\"boolean\"}\n",
        "load_4bit_models = True #@param {type:\"boolean\"}\n",
        "\n",
        "groupsize_128 = False #@param {type:\"boolean\"}\n",
        "load_in_8bit = False #@param {type:\"boolean\"}\n",
        "chat = True #@param {type:\"boolean\"}\n",
        "\n",
        "text_streaming = True #@param {type:\"boolean\"}\n",
        "activate_silero_text_to_speech = False #@param {type:\"boolean\"}\n",
        "activate_sending_pictures = False #@param {type:\"boolean\"}\n",
        "activate_character_bias = False #@param {type:\"boolean\"}\n",
        "chat_language = \"English\" # @param ['Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Azerbaijani', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Bulgarian', 'Catalan', 'Cebuano', 'Chinese (Simplified)', 'Chinese (Traditional)', 'Corsican', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Esperanto', 'Estonian', 'Finnish', 'French', 'Frisian', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hmong', 'Hungarian', 'Icelandic', 'Igbo', 'Indonesian', 'Irish', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Kurdish', 'Kyrgyz', 'Lao', 'Latin', 'Latvian', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Mongolian', 'Myanmar (Burmese)', 'Nepali', 'Norwegian', 'Nyanja (Chichewa)', 'Pashto', 'Persian', 'Polish', 'Portuguese (Portugal, Brazil)', 'Punjabi', 'Romanian', 'Russian', 'Samoan', 'Scots Gaelic', 'Serbian', 'Sesotho', 'Shona', 'Sindhi', 'Sinhala (Sinhalese)', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog (Filipino)', 'Tajik', 'Tamil', 'Telugu', 'Thai', 'Turkish', 'Ukrainian', 'Urdu', 'Uzbek', 'Vietnamese', 'Welsh', 'Xhosa', 'Yiddish', 'Yoruba', 'Zulu']\n",
        "\n",
        "activate_google_translate = (chat_language != \"English\")\n",
        "\n",
        "language_codes = {'Afrikaans': 'af', 'Albanian': 'sq', 'Amharic': 'am', 'Arabic': 'ar', 'Armenian': 'hy', 'Azerbaijani': 'az', 'Basque': 'eu', 'Belarusian': 'be', 'Bengali': 'bn', 'Bosnian': 'bs', 'Bulgarian': 'bg', 'Catalan': 'ca', 'Cebuano': 'ceb', 'Chinese (Simplified)': 'zh-CN', 'Chinese (Traditional)': 'zh-TW', 'Corsican': 'co', 'Croatian': 'hr', 'Czech': 'cs', 'Danish': 'da', 'Dutch': 'nl', 'English': 'en', 'Esperanto': 'eo', 'Estonian': 'et', 'Finnish': 'fi', 'French': 'fr', 'Frisian': 'fy', 'Galician': 'gl', 'Georgian': 'ka', 'German': 'de', 'Greek': 'el', 'Gujarati': 'gu', 'Haitian Creole': 'ht', 'Hausa': 'ha', 'Hawaiian': 'haw', 'Hebrew': 'iw', 'Hindi': 'hi', 'Hmong': 'hmn', 'Hungarian': 'hu', 'Icelandic': 'is', 'Igbo': 'ig', 'Indonesian': 'id', 'Irish': 'ga', 'Italian': 'it', 'Japanese': 'ja', 'Javanese': 'jw', 'Kannada': 'kn', 'Kazakh': 'kk', 'Khmer': 'km', 'Korean': 'ko', 'Kurdish': 'ku', 'Kyrgyz': 'ky', 'Lao': 'lo', 'Latin': 'la', 'Latvian': 'lv', 'Lithuanian': 'lt', 'Luxembourgish': 'lb', 'Macedonian': 'mk', 'Malagasy': 'mg', 'Malay': 'ms', 'Malayalam': 'ml', 'Maltese': 'mt', 'Maori': 'mi', 'Marathi': 'mr', 'Mongolian': 'mn', 'Myanmar (Burmese)': 'my', 'Nepali': 'ne', 'Norwegian': 'no', 'Nyanja (Chichewa)': 'ny', 'Pashto': 'ps', 'Persian': 'fa', 'Polish': 'pl', 'Portuguese (Portugal, Brazil)': 'pt', 'Punjabi': 'pa', 'Romanian': 'ro', 'Russian': 'ru', 'Samoan': 'sm', 'Scots Gaelic': 'gd', 'Serbian': 'sr', 'Sesotho': 'st', 'Shona': 'sn', 'Sindhi': 'sd', 'Sinhala (Sinhalese)': 'si', 'Slovak': 'sk', 'Slovenian': 'sl', 'Somali': 'so', 'Spanish': 'es', 'Sundanese': 'su', 'Swahili': 'sw', 'Swedish': 'sv', 'Tagalog (Filipino)': 'tl', 'Tajik': 'tg', 'Tamil': 'ta', 'Telugu': 'te', 'Thai': 'th', 'Turkish': 'tr', 'Ukrainian': 'uk', 'Urdu': 'ur', 'Uzbek': 'uz', 'Vietnamese': 'vi', 'Welsh': 'cy', 'Xhosa': 'xh', 'Yiddish': 'yi', 'Yoruba': 'yo', 'Zulu': 'zu'}\n",
        "\n",
        "%cd {repo_dir}\n",
        "# Applying the selected language and setting the prompt size to 2048\n",
        "# if 8bit mode is selected\n",
        "j = json.loads(open('settings-colab-template.json', 'r').read())\n",
        "j[\"google_translate-language string\"] = language_codes[chat_language]\n",
        "if load_in_8bit:\n",
        "  j[\"chat_prompt_size\"] = 2048\n",
        "with open('settings-colab.json', 'w') as f:\n",
        "  f.write(json.dumps(j, indent=4))\n",
        "\n",
        "params = set()\n",
        "if chat:\n",
        "  params.add('--chat')\n",
        "\n",
        "if load_in_8bit:\n",
        "  params.add('--load-in-8bit')\n",
        "#if auto_devices:\n",
        "#  params.add('--auto-devices')\n",
        "if load_4bit_models:\n",
        "  params.add('--wbits 4')\n",
        "\n",
        "if groupsize_128:\n",
        "  params.add('--groupsize 128')\n",
        "\n",
        "active_extensions = []\n",
        "if activate_sending_pictures:\n",
        "  active_extensions.append('send_pictures')\n",
        "if activate_character_bias:\n",
        "  active_extensions.append('character_bias')\n",
        "if activate_google_translate:\n",
        "  active_extensions.append('google_translate')\n",
        "if activate_silero_text_to_speech:\n",
        "  active_extensions.append('silero_tts')\n",
        "active_extensions.append('gallery')\n",
        "\n",
        "if len(active_extensions) > 0:\n",
        "  params.add(f'--extensions {\" \".join(active_extensions)}')\n",
        "\n",
        "if not text_streaming or activate_google_translate or activate_silero_text_to_speech:\n",
        "  params.add('--no-stream')\n",
        "if activate_character_bias:\n",
        "  params.add('--verbose')\n",
        "\n",
        "# Starting the web UI with tmux\n",
        "cmd = f\"tmux new -d python server.py --api  --loader exllama --max_seq_len 2048 --compress_pos_emb 1 --model {model_load}  --model_type LLaMa --settings settings-colab.json {' '.join(params)} \"#>/content/logs.txt    #2>&1\n",
        "print(cmd)\n",
        "#for guanaco --quant-type  nf4  fp4 gptq-for-llama\n",
        "#for falcon model --autogptq --trust-remote-code --groupsize 64\n",
        "!$cmd\n",
        "!rm -f /tmp/tmuxpipe && mkfifo /tmp/tmuxpipe && tmux pipe-pane -t 0 -o 'cat >> /tmp/tmuxpipe'\n",
        "!cat /tmp/tmuxpipe > /content/log.txt 2>&1 &\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "code",
        "id": "8rDMP2UBzg7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047cff55-b442-40c0-ce4c-357f1dd94fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-12 15:06:18 INFO:\u001b[32mLoading TheBloke_Vicuna-13B-1-3-SuperHOT-8K-GPTQ...\u001b[0m\n",
            "To create a public link, set `share=True` in `launch()`.\n",
            "2023-07-12 15:06:18 INFO:\u001b[32mLoading settings from settings-colab.json...\u001b[0m\n",
            "2023-07-12 15:06:18 INFO:\u001b[32mLoading TheBloke_Vicuna-13B-1-3-SuperHOT-8K-GPTQ...\u001b[0m\n",
            "2023-07-12 15:06:26 INFO:\u001b[32mLoaded the model in 7.24 seconds.\n",
            "\u001b[0m\n",
            "2023-07-12 15:06:26 INFO:\u001b[32mLoading the extension \"gallery\"...\u001b[0m\n",
            "Starting streaming server at ws://127.0.0.1:5005/api/v1/stream\n",
            "Starting API at http://127.0.0.1:5000/api\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        }
      ],
      "source": [
        "#@title 5. Logs - server is starting\n",
        "# update and wait until server is fully started\n",
        "\n",
        "import psutil, time\n",
        "from time import sleep\n",
        "import IPython\n",
        "from IPython.display import clear_output\n",
        "clear_output\n",
        "\n",
        "#check if proxy port is open\n",
        "while((5000 in [i.laddr.port for i in psutil.net_connections()]) != True):\n",
        "  sleep(5)\n",
        "  !tail -n 1  /content/log.txt\n",
        "\n",
        "!tail -n 10  /content/log.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "q6K14PG1XS27",
        "outputId": "30f0f26e-e538-4c72-a251-3c2d5e7aab35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "External link: https://jidetpx7c39-496ff2e9c6d22116-5001-colab.googleusercontent.com/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(5001, \"/\", \"100%\", \"400\", false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title 6. Simple UI - Make me summary\n",
        "\n",
        "import IPython\n",
        "from IPython.display import clear_output\n",
        "\n",
        "try:\n",
        "  import flask, flask_socketio\n",
        "except ImportError:\n",
        "  !pip install Flask flask-socketio eventlet gunicorn langchain==0.0.225\n",
        "  clear_output()\n",
        "\n",
        "import requests\n",
        "import asyncio\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "import threading\n",
        "import secrets\n",
        "import flask\n",
        "from flask import Flask, request, jsonify, render_template_string, session\n",
        "import flask_socketio\n",
        "from flask_socketio import SocketIO\n",
        "\n",
        "iport = 5001 # interface port\n",
        "from google.colab.output import eval_js\n",
        "print(\"External link:\",end=\" \")\n",
        "print(eval_js(f\"google.colab.kernel.proxyPort({iport})\"))\n",
        "from google.colab import output\n",
        "output.serve_kernel_port_as_iframe(iport)\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.logger.info(\"Starting...\")\n",
        "socketio = SocketIO(app)\n",
        "app.secret_key = secrets.token_hex(16)\n",
        "\n",
        "def log_line() :\n",
        "  with os.popen('tail -n 1 /content/log.txt') as pse:\n",
        "    for line in pse:\n",
        "      return line\n",
        "\n",
        "try:\n",
        "    import websockets\n",
        "except ImportError:\n",
        "    print(\"Websockets package not found. Make sure it's installed.\")\n",
        "\n",
        "!fuser -k {iport}/tcp  # close UI port\n",
        "\n",
        "\n",
        "#api1\n",
        "# For local streaming, the websockets are hosted without ssl - ws://\n",
        "HOST_stream = 'localhost:5005'\n",
        "URI_stream = f'ws://{HOST_stream}/api/v1/stream'\n",
        "\n",
        "# For reverse-proxied streaming, the remote will likely host with ssl - wss://\n",
        "# URI = 'wss://your-uri-here.trycloudflare.com/api/v1/stream'\n",
        "\n",
        "#api2 for one block reponse\n",
        "HOST = 'localhost:5000'\n",
        "URI = f'http://{HOST}/api/v1/generate'\n",
        "\n",
        "\n",
        "def generate(prompt, temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "    request = {\n",
        "        'prompt': prompt,\n",
        "        'max_new_tokens': 900,\n",
        "        'do_sample': True,\n",
        "        'temperature': temperature,\n",
        "        'top_p': top_p,\n",
        "        'typical_p': typical_p,\n",
        "        'repetition_penalty': repetition_penalty,\n",
        "        'top_k': top_k,\n",
        "        'min_length': 0,\n",
        "        'no_repeat_ngram_size': 0,\n",
        "        'num_beams': 1,\n",
        "        'penalty_alpha': 0,\n",
        "        'length_penalty': 1,\n",
        "        'early_stopping': False,\n",
        "        'seed': -1,\n",
        "        'add_bos_token': True,\n",
        "        'truncation_length': 2048,\n",
        "        'ban_eos_token': False,\n",
        "        'skip_special_tokens': True,\n",
        "        'stopping_strings': []\n",
        "      # 'custom_stopping_strings': \"You:\" ##for example\n",
        "    }\n",
        "\n",
        "    response = requests.post(URI, json=request)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()['results'][0]['text']\n",
        "\n",
        "    return result\n",
        "\n",
        "async def run(context, temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "    request = {\n",
        "        'prompt': context,\n",
        "        'max_new_tokens': 900,\n",
        "        'do_sample': True,\n",
        "        'temperature': temperature,\n",
        "        'top_p': top_p,\n",
        "        'typical_p': typical_p,\n",
        "        'repetition_penalty': repetition_penalty,\n",
        "        'top_k': top_k,\n",
        "        'min_length': 0,\n",
        "        'no_repeat_ngram_size': 0,\n",
        "        'num_beams': 1,\n",
        "        'penalty_alpha': 0,\n",
        "        'length_penalty': 1,\n",
        "        'early_stopping': False,\n",
        "        'seed': -1,\n",
        "        'add_bos_token': True,\n",
        "        'truncation_length': 2048,\n",
        "        'ban_eos_token': False,\n",
        "        'skip_special_tokens': True,\n",
        "        'stopping_strings': []\n",
        "    }\n",
        "\n",
        "    async with websockets.connect(URI_stream, ping_interval=None) as websocket:\n",
        "        await websocket.send(json.dumps(request))\n",
        "\n",
        "        #yield context  # Remove this if you just want to see the reply\n",
        "\n",
        "        while True:\n",
        "            incoming_data = await websocket.recv()\n",
        "            incoming_data = json.loads(incoming_data)\n",
        "\n",
        "            match incoming_data['event']:\n",
        "                case 'text_stream':\n",
        "                    yield incoming_data['text']\n",
        "                case 'stream_end':\n",
        "                    return\n",
        "\n",
        "response_apistream =\"\"\n",
        "\n",
        "async def print_response_stream(prompt, temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "    async for response in run(prompt, temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "        global response_apistream\n",
        "        response_apistream = response_apistream + response\n",
        "\n",
        "\n",
        "def stop_stream():\n",
        "    stop_url = f'http://{HOST}/api/v1/stop-stream'\n",
        "    response = requests.post(stop_url,json={})\n",
        "    #if response.status_code == 200:\n",
        "    #   print(\"Stream stopped successfully.\")\n",
        "    #else:\n",
        "    #    print(\"Failed to stop the stream.\")\n",
        "\n",
        "is_stream_running = False\n",
        "is_summary_running = False\n",
        "\n",
        "def api_stream(temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "    global question_text, is_stream_running\n",
        "    asyncio.run(print_response_stream(question_text, temperature, top_p, typical_p, repetition_penalty, top_k))\n",
        "\n",
        "\n",
        "def start_api_stream(temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "   global is_stream_running\n",
        "   if not is_stream_running:\n",
        "        t1 = threading.Thread(target=api_stream, args=(temperature, top_p, typical_p, repetition_penalty, top_k))\n",
        "        t1.start()\n",
        "        is_stream_running = True\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "def trim_string(input_string):\n",
        "    input_string = str(input_string)\n",
        "    trim_index = input_string.find('\" metadata=')\n",
        "    if trim_index != -1:  # If the phrase is found\n",
        "        return input_string[14:trim_index] #first string page_content\n",
        "    else:\n",
        "        return input_string  # If the phrase isn't found, return the original string\n",
        "\n",
        "def split_text_with_dot(text):\n",
        "  text_splitter = CharacterTextSplitter(chunk_size=4000, chunk_overlap=0,separator=\".\") # size in characters not tokens (1 token = 4 chars)\n",
        "  texts = text_splitter.split_text(text)                                                # good accuracy is at 1000 tokens context. Wider window makes accuracy less accurate.\n",
        "                                                                                        # standard window size for vicuna is 2048 tokens. With superhot and 24GVRAM can be:\n",
        "  text_splitter = CharacterTextSplitter(chunk_size=4000, chunk_overlap=0,separator=\" \") # llama-13b\tmax_seq_len = 8192, compress_pos_emb = 4\t6079 tokens\n",
        "                                                                                        # llama-30b\tmax_seq_len = 3584, compress_pos_emb = 2\t3100 tokens\n",
        "                                                                                        # also \"'max_new_tokens': 900\" param matters as it define output length of each chunk.\n",
        "                                                                                        # more detailed answer requires higer value\n",
        "  #split with space if with dot is not possible\n",
        "  texta = []\n",
        "  textb = []\n",
        "  for i in range(len(texts)):\n",
        "    texta = text_splitter.split_text(str(texts[i]))\n",
        "    textb.extend(texta)\n",
        "  return textb\n",
        "\n",
        "def wrap_lines(text, width):\n",
        "    words = text.split()\n",
        "    line = ''\n",
        "    result = []\n",
        "\n",
        "    for word in words:\n",
        "        if word.endswith(\"*\"):  #Separator\n",
        "            word = word[:-1]  # Remove the asterisk at the end of the word\n",
        "            line += word + ' '\n",
        "            result.append(line.strip())\n",
        "            line = '* '\n",
        "        elif len(line) + len(word) <= width:\n",
        "            line += word + ' '\n",
        "        else:\n",
        "            result.append(line.strip())\n",
        "            line = word + ' '\n",
        "\n",
        "    result.append(line.strip())\n",
        "    return '\\n'.join(result)\n",
        "\n",
        "def start_make_summary(texts, summary_rounds, temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "    global is_summary_running\n",
        "    if not is_stream_running:\n",
        "      t3 = threading.Thread(target=make_summary, args=(texts, summary_rounds, temperature, top_p, typical_p, repetition_penalty, top_k))\n",
        "      t3.start()\n",
        "      is_summary_running  = True\n",
        "\n",
        "def make_summary(texts, summary_rounds, temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "  global answer_text, is_summary_running, log_rounds, log_chunks\n",
        "  for i in range(summary_rounds):\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=4000, chunk_overlap=0,separator=\"\\n\")\n",
        "    sum_texts = \"\"\n",
        "    prompt=\"This is a conversation with your Assistant. The Assistant is very helpful and is eager to chat with you and answer your questions. You: Make a detail summary of the text in bullet points: \"\n",
        "    log_rounds=\"  Round \"+ str(i+1) + \" of \" + str(summary_rounds)\n",
        "    #log_chunks =\" tst\"\n",
        "    for i in range(len(texts)):\n",
        "      text = trim_string(texts[i])\n",
        "      texts[i] = prompt + text + \". Assistant:\"\n",
        "      log_chunks=\"Processing \"+ str(i+1) + \" of \" + str(len(texts)) + \" chunks\"\n",
        "      sum_texts += generate(str(texts[i]), temperature, top_p, typical_p, repetition_penalty, top_k)\n",
        "    texts = text_splitter.split_text(sum_texts)\n",
        "  answer_text = ''.join(texts)\n",
        "  answer_text = wrap_lines(answer_text, 200)\n",
        "  is_summary_running  = False\n",
        " # %cd /content/\n",
        " # with open(\"summary.txt\", \"w\") as w:\n",
        " #   w.writelines(answer_text)\n",
        "  #return str(texts)\n",
        "\n",
        "\n",
        "#example prompt\n",
        "question_text =\"This is a conversation with your Assistant. The Assistant is very helpful and is eager to chat with you and answer your questions. You: Tell me about yourself. Assistant:\"\n",
        "answer_text = \"\"\n",
        "start_time= \"\"\n",
        "file_name=\"\"\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def index():\n",
        "\n",
        "    if request.method == 'POST':\n",
        "        button_clicked = 'button_status' in request.form\n",
        "        stream_enabled = 'stream_enable' in request.form\n",
        "        summary_enabled = 'summary_enable' in request.form\n",
        "        summary_rounds = int(request.form.get('summary_rounds', session.get('summary_rounds', '1')))\n",
        "        temperature = float(request.form.get('temperature', session.get('temperature', '0.7')))\n",
        "        top_p = float(request.form.get('top_p', session.get('top_p', '0.1')))\n",
        "        typical_p = float(request.form.get('typical_p', session.get('typical_p', '1')))\n",
        "        repetition_penalty = float(request.form.get('repetition_penalty', session.get('repetition_penalty', '1.00')))\n",
        "        top_k = int(request.form.get('top_k', session.get('top_k', '40')))\n",
        "\n",
        "        global question_text, answer_text, response_apistream, is_stream_running, is_summary_running, start_time, file_name ,log_rounds, log_chunks, model_load\n",
        "        question_text = request.form.get('prompt', '')\n",
        "        start_time = request.form.get('startTime','')\n",
        "\n",
        "\n",
        "        # finish summary\n",
        "        if not button_clicked and not is_summary_running and summary_enabled: # summary has been finished\n",
        "           summary_enabled = False  # end of summary process\n",
        "        # start summary thread\n",
        "        if  not is_summary_running and summary_enabled: # if summary_enabled is true, browswer triggers post request every 5sec, to check if summary is finished by \"is_summary_running == False\"\n",
        "            text = split_text_with_dot(question_text)\n",
        "            start_make_summary(text, summary_rounds ,temperature, top_p, typical_p, repetition_penalty, top_k)\n",
        "            modified_form = request.form.copy()\n",
        "            modified_form['sendButton'] = 'Send Text'\n",
        "            button_clicked = False #summary procces triggered and started\n",
        "            file_name = request.form.get('fileInput','') + \" with \" + model_load # add used model name\n",
        "\n",
        "        # proceed summary\n",
        "        if  is_summary_running and summary_enabled:\n",
        "            answer_text = \"Generating summary...\"+ log_rounds + \" \" + log_chunks\n",
        "\n",
        "        # start & finish blocking response\n",
        "        if button_clicked and not stream_enabled and not summary_enabled:\n",
        "            answer_text = generate(question_text, temperature, top_p, typical_p, repetition_penalty, top_k)\n",
        "            modified_form = request.form.copy()\n",
        "            modified_form['sendButton'] = 'Send Text'\n",
        "            button_clicked = False  # end final blocking response\n",
        "\n",
        "        # finish stream\n",
        "        if response_apistream == answer_text and response_apistream != \"\":\n",
        "            button_clicked = False  # stream end\n",
        "            is_stream_running = False\n",
        "            response_apistream = \"\"\n",
        "        # start stream thread\n",
        "        if button_clicked and stream_enabled and not is_stream_running :\n",
        "            response_apistream = \"\"\n",
        "            start_api_stream(temperature, top_p, typical_p, repetition_penalty, top_k)  # Start stream\n",
        "        # manual stop stream\n",
        "        if button_clicked and request.form.get('sendButton', '') == \"   Stop    \":\n",
        "           stop_stream() # Stop stream\n",
        "           modified_form = request.form.copy()\n",
        "           modified_form['sendButton'] = 'Send Text'\n",
        "           response_apistream = \"\"\n",
        "           request.form = modified_form\n",
        "           button_clicked = False  # Stream end\n",
        "           is_stream_running = False\n",
        "        # proceed stream\n",
        "        if button_clicked and stream_enabled and not summary_enabled:\n",
        "            if response_apistream == \"\":\n",
        "                answer_text = \"Generating...\"\n",
        "            else:\n",
        "                answer_text = response_apistream  # Copy stream chunk\n",
        "\n",
        "        log_text = log_line()\n",
        "\n",
        "        session['temperature'] = temperature\n",
        "        session['top_p'] = top_p\n",
        "        session['typical_p'] = typical_p\n",
        "        session['repetition_penalty'] = repetition_penalty\n",
        "        session['top_k'] = top_k\n",
        "\n",
        "    else:\n",
        "        #question_text = request.form.get('prompt', '')\n",
        "        answer_text = ''\n",
        "        log_text = ''\n",
        "        stream_enabled = True\n",
        "        button_clicked = False\n",
        "        summary_enabled = False\n",
        "        summary_rounds = int(session.get('summary_rounds', '1'))\n",
        "        temperature = float(session.get('temperature', '0.7'))\n",
        "        top_p = float(session.get('top_p', '0.1'))\n",
        "        typical_p = float(session.get('typical_p', '1'))\n",
        "        repetition_penalty = float(session.get('repetition_penalty', '1.00'))\n",
        "        top_k = int(session.get('top_k', '40'))\n",
        "\n",
        "    return render_template_string('''\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Make Me Summary Colab UI</title>\n",
        "    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
        "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/FileSaver.js/2.0.0/FileSaver.min.js\"></script>\n",
        "    <script>\n",
        "\n",
        "        function clickSummary() {\n",
        "        $(\"#summary_enable\").prop(\"checked\", true);\n",
        "        $(\"#button_status\").prop(\"checked\", true); // trigger summary process by (button_status and summary_enable) at the same time\n",
        "        startTimeSum = new Date();\n",
        "        $(\"#startTime\").val(startTimeSum); // get date for summary timer\n",
        "        document.forms[0].submit();  // Send POST\n",
        "        }\n",
        "\n",
        "        function loadFile() {\n",
        "           var fileInput = document.getElementById('fileInput');\n",
        "           var questionTextarea = document.getElementById('question');\n",
        "           var file = fileInput.files[0];\n",
        "           var reader = new FileReader();\n",
        "           reader.onload = function(e) {\n",
        "             var contents = e.target.result;\n",
        "             questionTextarea.value = contents;\n",
        "          };\n",
        "          reader.readAsText(file);\n",
        "        }\n",
        "\n",
        "        function saveSummary() {\n",
        "          var summary = document.getElementById('answer').value;\n",
        "          var blob = new Blob([summary], { type: 'text/plain' });\n",
        "          var temperatureSlider = document.getElementById(\"temperature\");\n",
        "          var top_pSlider = document.getElementById(\"top_p\");\n",
        "          var typical_pSlider = document.getElementById(\"typical_p\");\n",
        "          var repetition_penaltySlider = document.getElementById(\"repetition_penalty\");\n",
        "          var top_k_Slider = document.getElementById(\"top_k\");\n",
        "          var summary_roundsSlider = document.getElementById(\"summary_rounds\");\n",
        "\n",
        "          var fileNameArea = document.getElementById('fileName');\n",
        "          var fileName = \"Summary of \" + fileNameArea.value + \" (rounds=\" + summary_roundsSlider.value + \" temp=\" + temperatureSlider.value + \" top_p=\" + top_pSlider.value + \" typ_p=\" + typical_pSlider.value + \" rep_pen=\" + repetition_penalty.value + \" top_k=\" + top_k.value + \").txt\"  ;\n",
        "\n",
        "          if (window.saveAs) {\n",
        "            window.saveAs(blob, fileName);\n",
        "            }  else {\n",
        "              var url = URL.createObjectURL(blob);\n",
        "              var link = document.createElement('a');\n",
        "              link.href = url;\n",
        "              link.download = fileName;\n",
        "              link.click();\n",
        "              URL.revokeObjectURL(url);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        function updateSliderValue(slider) {\n",
        "            var sliderId = slider.id;\n",
        "            var valueElement = document.getElementById(sliderId + 'Value');\n",
        "            valueElement.innerText = slider.value;\n",
        "        }\n",
        "\n",
        "        function simulateButtonClick() {         //work around to get data from colab web url, due o lack of suport of data requests like json etc.\n",
        "                document.forms[0].submit();  // Send POST in loop\n",
        "        }\n",
        "\n",
        "        function startTimer(period) {\n",
        "            setInterval(simulateButtonClick, period); // 2.5 seconds\n",
        "                 // document.forms[0].submit();  // Send POST first time after enabling stream\n",
        "        }\n",
        "//start doc ready function\n",
        "      $(document).ready(function(){\n",
        "\n",
        "            if (!$(\"#button_status\").is(\":checked\")) { // if not\n",
        "                 $(\"#sendButton\").val(\"Send Text\");\n",
        "            }\n",
        "\n",
        "            $(\"#loader\").hide();\n",
        "\n",
        "        var startTimearea = document.getElementById(\"startTime\");\n",
        "        var startTimeS = new Date(startTimearea.value);\n",
        "\n",
        "\n",
        "        setTimeout(function(){ //wait second delay for answer logs view\n",
        "         }, 1000);\n",
        "\n",
        "\n",
        "        // labels update and timer view\n",
        "        var FileNameAreaID = document.getElementById(\"fileName\");\n",
        "        var labelElementA = document.querySelector('label[for=\"answer\"]');\n",
        "        var labelElementQ = document.querySelector('label[for=\"question\"]');\n",
        "        var fileValue = FileNameAreaID.value;\n",
        "        if (FileNameAreaID.value.trim() !== \"\") {\n",
        "          labelElementQ.textContent = \"Question: Make me summary\";\n",
        "          labelElementA.textContent = \"Answer: \" + \" Summary of \" + fileValue;\n",
        "        }\n",
        "        var seconds;\n",
        "        var minutes;\n",
        "        var timerId = setInterval(function() {\n",
        "                  var currentTime = new Date();\n",
        "                  var elapsedTime = Math.round((currentTime - startTimeS) / 1000);\n",
        "                  seconds = elapsedTime % 60;\n",
        "                  minutes = Math.floor(elapsedTime / 60);\n",
        "                  if ($(\"#summary_enable\").is(\":checked\")){\n",
        "                      $(\"#answer\").val(\"Generating summary...  \" + minutes + \"m \" + seconds + \"s\");\n",
        "                  }\n",
        "\n",
        "                  if (!$(\"#summary_enable\").is(\":checked\") && FileNameAreaID.value.trim() !== \"\") {\n",
        "                    labelElementA.textContent = \"Answer: \" + \" Summary of \" + fileValue + \" Generated in \" + minutes + \" minutes \" + seconds + \" seconds\";\n",
        "                    clearInterval(timerId);\n",
        "                    saveSummary();\n",
        "                  }\n",
        "       }, 1000);\n",
        "\n",
        "//start submit\n",
        "             $(\"form\").submit(function(event){  // Send Text button function definition\n",
        "\n",
        "                $(\"#button_status\").prop(\"checked\", true); // button click\n",
        "\n",
        "                $(\"#loader\").show();\n",
        "                startTime = new Date();\n",
        "                $(\"#answer\").val(\"Generating...  0s\");\n",
        "                timer = setInterval(function() {\n",
        "                    var currentTime = new Date();\n",
        "                    var elapsedTime = Math.round((currentTime - startTime) / 1000);\n",
        "                    $(\"#answer\").val(\"Generating...  \" + elapsedTime + \"s\");\n",
        "                }, 1000);\n",
        "\n",
        "            });\n",
        "//end submit\n",
        "            if  ( $(\"#summary_enable\").is(\":checked\") ) {\n",
        "                 //event.preventDefault(); // Disable sending POST\n",
        "                 startTimer(5000); // run cyclic request for summary status\n",
        "                    }\n",
        "              if ($(\"#stream_enable\").is(\":checked\") && $(\"#button_status\").is(\":checked\")) {\n",
        "                 startTimer(2500); // run cyclic request for streamdata\n",
        "                   }\n",
        "        });\n",
        "//end doc ready function\n",
        "    </script>\n",
        "    <style>\n",
        "        #loader {\n",
        "            border: 5px solid #f3f3f3;\n",
        "            border-top: 5px solid #3498db;\n",
        "            border-right: 5px solid #3498db;\n",
        "            border-bottom: 5px solid #f3f3f3;\n",
        "            border-left: 5px solid #f3f3f3;\n",
        "            border-radius: 50%;\n",
        "            width: 10px;\n",
        "            height: 10px;\n",
        "            animation: spin 1.5s linear infinite;\n",
        "        }\n",
        "\n",
        "        @keyframes spin {\n",
        "            0% { transform: rotate(0deg); }\n",
        "            100% { transform: rotate(360deg); }\n",
        "        }\n",
        "\n",
        "        .hidden-checkbox {\n",
        "           position: absolute;\n",
        "           left: -9999px;\n",
        "        }\n",
        "      #startTime {\n",
        "         display: none;\n",
        "      }\n",
        "      #fileName {\n",
        "         display: none;\n",
        "      }\n",
        "      .button-container {\n",
        "        border: 1px solid #aaa;\n",
        "        padding: 8px;\n",
        "        border-radius: 5px;\n",
        "        display: inline-block;\n",
        "}\n",
        "\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "    <form method=\"POST\">\n",
        "        <label for=\"question\">Question:</label><br>\n",
        "        <textarea id=\"question\" name=\"prompt\" cols=\"160\" rows=\"3\">{{ question_text }}</textarea><br>\n",
        "        <label for=\"answer\">Answer:</label><br>\n",
        "        <textarea id=\"answer\" name=\"answer\" cols=\"160\" rows=\"9\">{{ answer_text }}</textarea>\n",
        "        <textarea id=\"startTime\" name=\"startTime\" cols=\"15\" rows=\"1\">{{ start_time }}</textarea>\n",
        "        <textarea id=\"fileName\" name=\"fileName\" cols=\"15\" rows=\"1\">{{ file_name }}</textarea><br>\n",
        "\n",
        "        <input type=\"submit\" id=\"sendButton\" name=\"sendButton\" value=\"   Stop    \" \">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp\n",
        "        <div class=\"button-container\">\n",
        "        <input type=\"file\" id=\"fileInput\" name=\"fileInput\" onchange=\"loadFile()\" accept=\".txt\" placeholder=\"Choose a file\">\n",
        "        <button type=\"button\" id=\"summaryButton\" onclick=\"clickSummary()\">Make summary</button> &nbsp&nbsp&nbsp\n",
        "        <label for=\"summary_rounds\">Summary rounds:</label>\n",
        "        <input type=\"range\" id=\"summary_rounds\" name=\"summary_rounds\" min=\"1\" max=\"5\" step=\"1\" value=\"{{ summary_rounds }}\" oninput=\"updateSliderValue(this)\">\n",
        "        <span id=\"summary_roundsValue\">{{ summary_rounds }}</span>&nbsp&nbsp&nbsp\n",
        "        </div>\n",
        "        <br><br>\n",
        "        <input type=\"checkbox\" id=\"stream_enable\" name=\"stream_enable\" {% if stream_enabled %}checked{% endif %}>\n",
        "        <label for=\"stream_enable\">Enable streaming</label>&nbsp&nbsp\n",
        "        <input type=\"checkbox\" id=\"summary_enable\" name=\"summary_enable\" class=\"hidden-checkbox\" {% if summary_enabled %}checked{% endif %}>\n",
        "        <br><br>&nbsp&nbsp\n",
        "        <label for=\"temperature\">Temperature:</label>\n",
        "        <input type=\"range\" id=\"temperature\" name=\"temperature\" min=\"0\" max=\"2\" step=\"0.01\" value=\"{{ temperature }}\" oninput=\"updateSliderValue(this)\">\n",
        "        <span id=\"temperatureValue\">{{ temperature }}</span>&nbsp&nbsp\n",
        "        <label for=\"top_p\">Top P:</label>\n",
        "        <input type=\"range\" id=\"top_p\" name=\"top_p\" min=\"0\" max=\"1\" step=\"0.01\" value=\"{{ top_p }}\" oninput=\"updateSliderValue(this)\">\n",
        "        <span id=\"top_pValue\">{{ top_p }}</span>&nbsp&nbsp\n",
        "        <label for=\"typical_p\">Typical P:</label>\n",
        "        <input type=\"range\" id=\"typical_p\" name=\"typical_p\" min=\"0\" max=\"1\" step=\"0.01\" value=\"{{ typical_p }}\" oninput=\"updateSliderValue(this)\">\n",
        "        <span id=\"typical_pValue\">{{ typical_p }}</span>&nbsp&nbsp\n",
        "        <label for=\"repetition_penalty\">Repetition Penalty:</label>\n",
        "        <input type=\"range\" id=\"repetition_penalty\" name=\"repetition_penalty\" min=\"0\" max=\"1.5\" step=\"0.01\" value=\"{{ repetition_penalty }}\" oninput=\"updateSliderValue(this)\">\n",
        "        <span id=\"repetition_penaltyValue\">{{ repetition_penalty }}</span>&nbsp&nbsp\n",
        "        <label for=\"top_k\">Top K:</label>\n",
        "        <input type=\"range\" id=\"top_k\" name=\"top_k\" min=\"1\" max=\"200\" step=\"1\" value=\"{{ top_k }}\" oninput=\"updateSliderValue(this)\">\n",
        "        <span id=\"top_kValue\">{{ top_k }}</span>&nbsp&nbsp\n",
        "        <input type=\"checkbox\" id=\"button_status\" name=\"button_status\" class=\"hidden-checkbox\" {% if button_clicked %}checked{% endif %}> <br>\n",
        "        <p>{{ log_text }}</p>\n",
        "        <div id=\"loader\"></div>\n",
        "    </form>\n",
        "</body>\n",
        "</html>\n",
        "    ''', question_text=question_text, answer_text=answer_text, log_text=log_text, stream_enabled=stream_enabled, summary_enabled=summary_enabled, button_clicked=button_clicked, summary_rounds=summary_rounds, temperature=temperature, top_p=top_p, typical_p=typical_p, repetition_penalty=repetition_penalty, top_k=top_k, start_time=start_time, file_name=file_name)\n",
        "\n",
        "#def socketio_thread():\n",
        "socketio.run(app, port=iport)\n",
        "\n",
        "#t2 = threading.Thread(target=socketio_thread, args=())\n",
        "#t2.start()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
