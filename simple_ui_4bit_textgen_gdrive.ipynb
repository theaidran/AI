{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theaidran/AI/blob/main/simple_ui_4bit_textgen_gdrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFQl6-FjSYtY"
      },
      "source": [
        "# LLM text generation notebook for Google Colab\n",
        "\n",
        "This notebook uses [https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) to run conversational models in chat mode. Find my latest version [here](https://github.com/eucdee/AI)\n",
        "\n",
        "‚ñ∂‚è©Run all the cells and a public gradio URL will appear at the bottom in around 5 minutes.ü§ûüê±‚Äçüë§\n",
        "\n",
        "https://status.gradio.app/\n",
        "\n",
        "## Parameters\n",
        "\n",
        "* **save_logs_to_google_drive**: saves your chat logs, characters, and softprompts to Google Drive automatically, so that they will persist across sessions.\n",
        "* **text_streaming**: streams the text output in real time instead of waiting for the full response to be completed.\n",
        "* **cai_chat**: makes the interface look like Character.AI. Otherwise, it looks like a standard WhatsApp-like chat.\n",
        "* **load_in_8bit**: loads the model with 8-bit precision, reducing the GPU memory usage by half. This allows you to use the full 2048 prompt length without running out of memory, at a small accuracy and speed cost.\n",
        "* **activate_silero_text_to_speech**: responses will be audios instead of text. There are 118 voices available (`en_0` to `en_117`), which can be set in the \"Extensions\" tab of the interface. You can find samples here: [Silero samples](https://oobabooga.github.io/silero-samples/).\n",
        "* **activate_sending_pictures**: adds a menu for sending pictures to the bot, which are automatically captioned using BLIP.\n",
        "* **activate_character_bias**: an extension that adds an user-defined, hidden string at the beginning of the bot's reply with the goal of biasing the rest of the response.\n",
        "* **chat_language**: if different than English, activates automatic translation using Google Translate, allowing you to communicate with the bot in a different language.\n",
        "\n",
        "## Updates\n",
        "\n",
        "* check [README](https://github.com/eucdee/AI/blob/main/README.md) on github for Updates\n",
        "\n",
        "\n",
        "## Characters\n",
        "\n",
        "You can use the following websites to create characters compatible with this web UI:\n",
        "\n",
        "* [JSON character creator](https://oobabooga.github.io/character-creator.html)\n",
        "* [AI Character Editor](https://zoltanai.github.io/character-editor/)\n",
        "\n",
        "## Credits\n",
        "\n",
        "Based on the [original notebook by 81300](https://colab.research.google.com/github/81300/AI-Notebooks/blob/main/Colab-TextGen-GPU.ipynb).\n",
        "\n",
        "Forked from [Philio](https://github.com/pcrii/Philo-Colab-Collection/blob/main/4bit_TextGen_Gdrive.ipynb). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "f7TVVj_z4flw",
        "outputId": "5306594b-9da5-4b35-95f1-f34d26a6204b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LGQ8BiMuXMDG",
        "outputId": "214400c3-becb-4766-a4fe-f0b3cd787d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished\n",
            "Available Models\n",
            "['config.yaml', 'place-your-models-here.txt']\n"
          ]
        }
      ],
      "source": [
        "#@title 2. Install the web UI\n",
        "#remember gradio is currently held back\n",
        "save_logs_to_google_drive = False #@param {type:\"boolean\"} \n",
        "save_everything_to_google_drive = False #@param {type:\"boolean\"} \n",
        "#@markdown remember these models are large and free Gdrive is only 15Ggb <br>\n",
        "install_gptq = True #@param {type:\"boolean\"}\n",
        "#@markdown Install GPTQ-for-LLaMa for 4bit quantized models requiring --wbits 4\n",
        "from IPython.display import clear_output\n",
        "if save_logs_to_google_drive:\n",
        "  import os\n",
        "  import shutil\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  base_folder = '/content/drive/MyDrive'\n",
        "\n",
        "if save_everything_to_google_drive:\n",
        "    import os\n",
        "    import shutil\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    base_folder = '/content/drive/MyDrive'\n",
        "    repo_dir = '/content/drive/MyDrive/text-generation-webui'\n",
        "    model_dir = '/content/drive/MyDrive/text-generation-webui/models'\n",
        "    gptq_dir = '/content/drive/MyDrive/text-generation-webui/repositories/GPTQ-for-LLaMa'\n",
        "    if os.path.exists(repo_dir):\n",
        "        %cd {repo_dir}\n",
        "        !git pull\n",
        "    else:\n",
        "        %cd /content/drive/MyDrive/\n",
        "        !git clone https://github.com/oobabooga/text-generation-webui\n",
        "\n",
        "else:\n",
        "    model_dir = '/content/text-generation-webui/models'\n",
        "    repo_dir = '/content/text-generation-webui'\n",
        "    %cd /content\n",
        "    !git clone https://github.com/oobabooga/text-generation-webui\n",
        "\n",
        "\n",
        "\n",
        "if save_logs_to_google_drive:\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data\"):\n",
        "    os.mkdir(f\"{base_folder}/oobabooga-data\")\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data/logs\"):\n",
        "    os.mkdir(f\"{base_folder}/oobabooga-data/logs\")\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data/softprompts\"):\n",
        "    os.mkdir(f\"{base_folder}/oobabooga-data/softprompts\")\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data/characters\"):\n",
        "    shutil.move(\"text-generation-webui/characters\", f\"{base_folder}/oobabooga-data/characters\")\n",
        "  else:\n",
        "    !rm -r \"text-generation-webui/characters\"\n",
        "    \n",
        "  !rm -r \"text-generation-webui/softprompts\"\n",
        "  !ln -s \"$base_folder/oobabooga-data/logs\" \"text-generation-webui/logs\"\n",
        "  !ln -s \"$base_folder/oobabooga-data/softprompts\" \"text-generation-webui/softprompts\"\n",
        "  !ln -s \"$base_folder/oobabooga-data/characters\" \"text-generation-webui/characters\"\n",
        "\n",
        "else:\n",
        "  !mkdir text-generation-webui/logs\n",
        "\n",
        "!ln -s text-generation-webui/logs .\n",
        "!ln -s text-generation-webui/characters .\n",
        "!ln -s text-generation-webui/models .\n",
        "%rm -r sample_data\n",
        "%cd text-generation-webui\n",
        "!wget https://raw.githubusercontent.com/pcrii/Philo-Colab-Collection/main/settings-colab-template.json -O settings-colab-template.json\n",
        "\n",
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "!pip install -r extensions/google_translate/requirements.txt\n",
        "!pip install -r extensions/silero_tts/requirements.txt\n",
        "print(f\"\\033[1;32;1m\\n --> If you see a warning about \\\"pydevd_plugins\\\", just ignore it and move on to Step 3. There is no need to restart the runtime.\\n\\033[0;37;0m\")\n",
        "\n",
        "if install_gptq:\n",
        "    if save_everything_to_google_drive:\n",
        "        if os.path.exists(gptq_dir):\n",
        "            %cd {gptq_dir}\n",
        "            !git pull\n",
        "            !pip install ninja\n",
        "            !pip install -r requirements.txt\n",
        "            !python setup_cuda.py install\n",
        "\n",
        "        else:\n",
        "            !mkdir /content/drive/MyDrive/text-generation-webui/repositories\n",
        "            %cd /content/drive/MyDrive/text-generation-webui/repositories\n",
        "            !git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\n",
        "            !ln -s GPTQ-for-LLaMa text-generation-webui/repositories/GPTQ-for-LLaMa\n",
        "            %cd GPTQ-for-LLaMa\n",
        "            !pip install ninja\n",
        "            !pip install -r requirements.txt\n",
        "            !python setup_cuda.py install\n",
        "    else:\n",
        "        %mkdir /content/text-generation-webui/repositories/\n",
        "        %cd /content/text-generation-webui/repositories/\n",
        "        !git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\n",
        "        !mkdir -p text-generation-webui/repositories\n",
        "        !ln -s GPTQ-for-LLaMa text-generation-webui/repositories/GPTQ-for-LLaMa\n",
        "        %cd GPTQ-for-LLaMa\n",
        "        !pip install ninja\n",
        "        !pip install -r requirements.txt\n",
        "        !python setup_cuda.py install\n",
        "clear_output()\n",
        "print(\"Finished\")\n",
        "if save_logs_to_google_drive or save_everything_to_google_drive:\n",
        "    drive_NOT_mounted = False\n",
        "else:\n",
        "    drive_NOT_mounted = True\n",
        "\n",
        "if drive_NOT_mounted:\n",
        "  import os\n",
        "print(\"Available Models\")\n",
        "print(os.listdir(model_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7t4QfXf4U9p",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 3. Download Model\n",
        "#@markdown you can insert any huggingface model in Organization/model format\n",
        "model_download = \"TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g\" #@param [ \"TheBloke/stable-vicuna-13B-GPTQ\", \"anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g\", \"anon8231489123/vicuna-13b-GPTQ-4bit-128g\", \"TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g\", \"reeducator/vicuna-13b-free\", \"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\", \"Aitrepreneur/wizardLM-7B-GPTQ-4bit-128g\", \"TheBloke/wizardLM-7B-GPTQ\", \"gozfarb/oasst-llama13b-4bit-128g\", \"catalpa/codecapybara-4bit-128g-gptq\", \"mzedp/dolly-v2-12b-GPTQ-4bit-128g\", \"autobots/pythia-12b-gptqv2-4bit\", \"TheBloke/medalpaca-13B-GPTQ-4bit\", \"TheBloke/gpt4-alpaca-lora-13B-GPTQ-4bit-128g\"] {allow-input: true}\n",
        "#@markdown remember these models are large and free Gdrive is only 15Ggb <br>\n",
        "\n",
        "#@markdown  Model type: Safetensors or Pickletensors\n",
        "Safetensor = False #@param {type:\"boolean\"}\n",
        "\n",
        "%cd /content/text-generation-webui\n",
        "with open(\"download-model.py\") as r:\n",
        "  if Safetensor == False:\n",
        "    text = r.read().replace(\"if classifications[i] in ['pytorch', 'pt']:\", \"if classifications[i] in ['safetensors']:\")\n",
        "  else:\n",
        "    text = r.read().replace(\"if classifications[i] in ['safetensors']:\", \"if classifications[i] in ['pytorch', 'pt']:\")\n",
        "with open(\"download-model.py\", \"w\") as w:\n",
        "  w.write(text) \n",
        "\n",
        "#@markdown  if box is not checked Pickletensors model will be downloaded if available. Safetensor is always loaded as default in webui\n",
        "%cd {repo_dir}\n",
        "!python download-model.py {model_download}\n",
        "#this lists directorys from your model folder you can copy the name provided for the model you want for use in the the next cell\n",
        "!rm {model_dir}/place-your-models-here.txt\n",
        "#clear_output()\n",
        "if save_logs_to_google_drive or save_everything_to_google_drive:\n",
        "    drive_NOT_mounted = False\n",
        "else:\n",
        "    drive_NOT_mounted = True\n",
        "\n",
        "if drive_NOT_mounted:\n",
        "  import os\n",
        "print(\"Available Models\")\n",
        "print(os.listdir(model_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKuocueuXnm5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 4. Launch\n",
        "import json\n",
        "\n",
        "#Close server if is running\n",
        "!pkill -f -e -c server.py\n",
        "\n",
        "#@markdown if you dont know what to enter the previous cell should have printed available inputs <br> paste it here\n",
        "model_load = \"TheBloke_vicuna-13B-1.1-GPTQ-4bit-128g\" #@param {type:\"string\"}\n",
        "# Parameters\n",
        "#auto_devices = False #@param {type:\"boolean\"}\n",
        "load_4bit_models = False #@param {type:\"boolean\"}\n",
        "\n",
        "groupsize_128 = False #@param {type:\"boolean\"}\n",
        "load_in_8bit = False #@param {type:\"boolean\"}\n",
        "chat = True #@param {type:\"boolean\"}\n",
        "\n",
        "text_streaming = True #@param {type:\"boolean\"}\n",
        "activate_silero_text_to_speech = False #@param {type:\"boolean\"}\n",
        "activate_sending_pictures = False #@param {type:\"boolean\"}\n",
        "activate_character_bias = False #@param {type:\"boolean\"}\n",
        "chat_language = \"English\" # @param ['Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Azerbaijani', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Bulgarian', 'Catalan', 'Cebuano', 'Chinese (Simplified)', 'Chinese (Traditional)', 'Corsican', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Esperanto', 'Estonian', 'Finnish', 'French', 'Frisian', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hmong', 'Hungarian', 'Icelandic', 'Igbo', 'Indonesian', 'Irish', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Kurdish', 'Kyrgyz', 'Lao', 'Latin', 'Latvian', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Mongolian', 'Myanmar (Burmese)', 'Nepali', 'Norwegian', 'Nyanja (Chichewa)', 'Pashto', 'Persian', 'Polish', 'Portuguese (Portugal, Brazil)', 'Punjabi', 'Romanian', 'Russian', 'Samoan', 'Scots Gaelic', 'Serbian', 'Sesotho', 'Shona', 'Sindhi', 'Sinhala (Sinhalese)', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog (Filipino)', 'Tajik', 'Tamil', 'Telugu', 'Thai', 'Turkish', 'Ukrainian', 'Urdu', 'Uzbek', 'Vietnamese', 'Welsh', 'Xhosa', 'Yiddish', 'Yoruba', 'Zulu']\n",
        "\n",
        "activate_google_translate = (chat_language != \"English\")\n",
        "\n",
        "language_codes = {'Afrikaans': 'af', 'Albanian': 'sq', 'Amharic': 'am', 'Arabic': 'ar', 'Armenian': 'hy', 'Azerbaijani': 'az', 'Basque': 'eu', 'Belarusian': 'be', 'Bengali': 'bn', 'Bosnian': 'bs', 'Bulgarian': 'bg', 'Catalan': 'ca', 'Cebuano': 'ceb', 'Chinese (Simplified)': 'zh-CN', 'Chinese (Traditional)': 'zh-TW', 'Corsican': 'co', 'Croatian': 'hr', 'Czech': 'cs', 'Danish': 'da', 'Dutch': 'nl', 'English': 'en', 'Esperanto': 'eo', 'Estonian': 'et', 'Finnish': 'fi', 'French': 'fr', 'Frisian': 'fy', 'Galician': 'gl', 'Georgian': 'ka', 'German': 'de', 'Greek': 'el', 'Gujarati': 'gu', 'Haitian Creole': 'ht', 'Hausa': 'ha', 'Hawaiian': 'haw', 'Hebrew': 'iw', 'Hindi': 'hi', 'Hmong': 'hmn', 'Hungarian': 'hu', 'Icelandic': 'is', 'Igbo': 'ig', 'Indonesian': 'id', 'Irish': 'ga', 'Italian': 'it', 'Japanese': 'ja', 'Javanese': 'jw', 'Kannada': 'kn', 'Kazakh': 'kk', 'Khmer': 'km', 'Korean': 'ko', 'Kurdish': 'ku', 'Kyrgyz': 'ky', 'Lao': 'lo', 'Latin': 'la', 'Latvian': 'lv', 'Lithuanian': 'lt', 'Luxembourgish': 'lb', 'Macedonian': 'mk', 'Malagasy': 'mg', 'Malay': 'ms', 'Malayalam': 'ml', 'Maltese': 'mt', 'Maori': 'mi', 'Marathi': 'mr', 'Mongolian': 'mn', 'Myanmar (Burmese)': 'my', 'Nepali': 'ne', 'Norwegian': 'no', 'Nyanja (Chichewa)': 'ny', 'Pashto': 'ps', 'Persian': 'fa', 'Polish': 'pl', 'Portuguese (Portugal, Brazil)': 'pt', 'Punjabi': 'pa', 'Romanian': 'ro', 'Russian': 'ru', 'Samoan': 'sm', 'Scots Gaelic': 'gd', 'Serbian': 'sr', 'Sesotho': 'st', 'Shona': 'sn', 'Sindhi': 'sd', 'Sinhala (Sinhalese)': 'si', 'Slovak': 'sk', 'Slovenian': 'sl', 'Somali': 'so', 'Spanish': 'es', 'Sundanese': 'su', 'Swahili': 'sw', 'Swedish': 'sv', 'Tagalog (Filipino)': 'tl', 'Tajik': 'tg', 'Tamil': 'ta', 'Telugu': 'te', 'Thai': 'th', 'Turkish': 'tr', 'Ukrainian': 'uk', 'Urdu': 'ur', 'Uzbek': 'uz', 'Vietnamese': 'vi', 'Welsh': 'cy', 'Xhosa': 'xh', 'Yiddish': 'yi', 'Yoruba': 'yo', 'Zulu': 'zu'}\n",
        "\n",
        "%cd {repo_dir}\n",
        "# Applying the selected language and setting the prompt size to 2048\n",
        "# if 8bit mode is selected\n",
        "j = json.loads(open('settings-colab-template.json', 'r').read())\n",
        "j[\"google_translate-language string\"] = language_codes[chat_language]\n",
        "if load_in_8bit:\n",
        "  j[\"chat_prompt_size\"] = 2048\n",
        "with open('settings-colab.json', 'w') as f:\n",
        "  f.write(json.dumps(j, indent=4))\n",
        "\n",
        "params = set()\n",
        "if chat:\n",
        "  params.add('--chat')\n",
        "\n",
        "if load_in_8bit:\n",
        "  params.add('--load-in-8bit')\n",
        "#if auto_devices:\n",
        "#  params.add('--auto-devices')\n",
        "if load_4bit_models:\n",
        "  params.add('--wbits 4')\n",
        "\n",
        "if groupsize_128:\n",
        "  params.add('--groupsize 128')\n",
        "\n",
        "active_extensions = []\n",
        "if activate_sending_pictures:\n",
        "  active_extensions.append('send_pictures')\n",
        "if activate_character_bias:\n",
        "  active_extensions.append('character_bias')\n",
        "if activate_google_translate:\n",
        "  active_extensions.append('google_translate')\n",
        "if activate_silero_text_to_speech:\n",
        "  active_extensions.append('silero_tts')\n",
        "active_extensions.append('gallery')\n",
        "\n",
        "if len(active_extensions) > 0:\n",
        "  params.add(f'--extensions {\" \".join(active_extensions)}')\n",
        "\n",
        "if not text_streaming or activate_google_translate or activate_silero_text_to_speech:\n",
        "  params.add('--no-stream')\n",
        "if activate_character_bias:\n",
        "  params.add('--verbose')\n",
        "\n",
        "# Starting the web UI with tmux\n",
        "cmd = f\"tmux new -d python server.py --share  --api --autogptq --model {model_load}  --model_type LLaMa --settings settings-colab.json {' '.join(params)} \"#>/content/logs.txt    #2>&1 \n",
        "print(cmd) \n",
        "#for guanaco --quant_type  nf4  fp4\n",
        "#for falcon model --autogptq --trust-remote-code --groupsize 64\n",
        "!$cmd\n",
        "!rm -f /tmp/tmuxpipe && mkfifo /tmp/tmuxpipe && tmux pipe-pane -t 0 -o 'cat >> /tmp/tmuxpipe'\n",
        "!cat /tmp/tmuxpipe > /content/log.txt 2>&1 &\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rDMP2UBzg7I",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20fac096-ae10-4564-c0f8-7194974875aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:\u001b[33mCUDA extension not installed.\u001b[0m\n",
            "WARNING:\u001b[33mCUDA extension not installed.\u001b[0m\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "INFO:\u001b[32mLoading TheBloke_vicuna-13B-1.1-GPTQ-4bit-128g...\u001b[0m\n",
            "INFO:\u001b[32mThe AutoGPTQ params are: {'model_basename': 'vicuna-13B-1.1-GPTQ-4bit-128g.compat.no-act-order', 'device': 'cuda:0', 'use_triton': False, 'use_safetensors': False, 'trust_remote_code': False, 'max_memory': None, 'quantize_config': None}\u001b[0m\n",
            "WARNING:\u001b[33mCUDA extension not installed.\u001b[0m\n",
            "WARNING:\u001b[33mskip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\u001b[0m\n",
            "INFO:\u001b[32mLoaded the model in 10.23 seconds.\n",
            "\u001b[0m\n",
            "INFO:\u001b[32mLoading the extension \"gallery\"...\u001b[0m\n",
            "Starting streaming server at ws://127.0.0.1:5005/api/v1/stream\n",
            "Starting API at http://127.0.0.1:5000/api\n",
            "Running on local URL:  http://127.0.0.1:7860\n"
          ]
        }
      ],
      "source": [
        "#@title 5. Logs - server is starting\n",
        "# update and wait until server is fully started\n",
        "\n",
        "import psutil, time \n",
        "from time import sleep\n",
        "import IPython\n",
        "from IPython.display import clear_output \n",
        "clear_output \n",
        "\n",
        "#check if proxy port is open\n",
        "while((5000 in [i.laddr.port for i in psutil.net_connections()]) != True):\n",
        "  sleep(5)\n",
        "  !tail -n 1  /content/log.txt\n",
        "\n",
        "!tail -n 10  /content/log.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q6K14PG1XS27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "5b8e1852-e9b1-4036-eb7f-571e6dfdf2be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "External link: https://tn69zi7ee0l-496ff2e9c6d22116-5001-colab.googleusercontent.com/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(5001, \"/\", \"100%\", \"400\", false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title 6. Simple UI \n",
        "\n",
        "import IPython\n",
        "from IPython.display import clear_output \n",
        "\n",
        "try: \n",
        "  import flask, flask_socketio\n",
        "except ImportError:\n",
        "  !pip install Flask flask-socketio eventlet gunicorn\n",
        "  clear_output()\n",
        "\n",
        "import requests\n",
        "import asyncio\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "import threading\n",
        "import secrets\n",
        "import flask\n",
        "from flask import Flask, request, jsonify, render_template_string, session\n",
        "import flask_socketio\n",
        "from flask_socketio import SocketIO\n",
        "\n",
        "iport = 5001 # interface port\n",
        "from google.colab.output import eval_js\n",
        "print(\"External link:\",end=\" \")\n",
        "print(eval_js(f\"google.colab.kernel.proxyPort({iport})\"))\n",
        "from google.colab import output\n",
        "output.serve_kernel_port_as_iframe(iport)\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.logger.info(\"Starting...\") \n",
        "socketio = SocketIO(app)\n",
        "app.secret_key = secrets.token_hex(16)\n",
        "\n",
        "def log_line() :\n",
        "  with os.popen('tail -n 1 /content/log.txt') as pse:\n",
        "    for line in pse:\n",
        "      return line\n",
        "\n",
        "try:\n",
        "    import websockets\n",
        "except ImportError:\n",
        "    print(\"Websockets package not found. Make sure it's installed.\")\n",
        "\n",
        "!fuser -k {iport}/tcp  # close UI port   \n",
        "\n",
        "\n",
        "#api1\n",
        "# For local streaming, the websockets are hosted without ssl - ws://\n",
        "HOST_stream = 'localhost:5005'\n",
        "URI_stream = f'ws://{HOST_stream}/api/v1/stream'\n",
        "\n",
        "# For reverse-proxied streaming, the remote will likely host with ssl - wss://\n",
        "# URI = 'wss://your-uri-here.trycloudflare.com/api/v1/stream'\n",
        "\n",
        "#api2 for one block reponse\n",
        "HOST = 'localhost:5000'\n",
        "URI = f'http://{HOST}/api/v1/generate'\n",
        "\n",
        "\n",
        "def generate(prompt, temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "    request = {\n",
        "        'prompt': prompt,\n",
        "        'max_new_tokens': 1000,\n",
        "        'do_sample': True,\n",
        "        'temperature': temperature,\n",
        "        'top_p': top_p,\n",
        "        'typical_p': typical_p,\n",
        "        'repetition_penalty': repetition_penalty,\n",
        "        'top_k': top_k,\n",
        "        'min_length': 0,\n",
        "        'no_repeat_ngram_size': 0,\n",
        "        'num_beams': 1,\n",
        "        'penalty_alpha': 0,\n",
        "        'length_penalty': 1,\n",
        "        'early_stopping': False,\n",
        "        'seed': -1,\n",
        "        'add_bos_token': True,\n",
        "        'truncation_length': 2048,\n",
        "        'ban_eos_token': False,\n",
        "        'skip_special_tokens': True,\n",
        "        'stopping_strings': []\n",
        "      # 'custom_stopping_strings': \"You:\" ##for example \n",
        "    }\n",
        "\n",
        "    response = requests.post(URI, json=request)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()['results'][0]['text']\n",
        "    \n",
        "    return result\n",
        "\n",
        "async def run(context, temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "    request = {\n",
        "        'prompt': context,\n",
        "        'max_new_tokens': 1000,\n",
        "        'do_sample': True,\n",
        "        'temperature': temperature,\n",
        "        'top_p': top_p,\n",
        "        'typical_p': typical_p,\n",
        "        'repetition_penalty': repetition_penalty,\n",
        "        'top_k': top_k,\n",
        "        'min_length': 0,\n",
        "        'no_repeat_ngram_size': 0,\n",
        "        'num_beams': 1,\n",
        "        'penalty_alpha': 0,\n",
        "        'length_penalty': 1,\n",
        "        'early_stopping': False,\n",
        "        'seed': -1,\n",
        "        'add_bos_token': True,\n",
        "        'truncation_length': 2048,\n",
        "        'ban_eos_token': False,\n",
        "        'skip_special_tokens': True,\n",
        "        'stopping_strings': []\n",
        "    }\n",
        "\n",
        "    async with websockets.connect(URI_stream, ping_interval=None) as websocket:\n",
        "        await websocket.send(json.dumps(request))\n",
        "\n",
        "        #yield context  # Remove this if you just want to see the reply\n",
        "\n",
        "        while True:\n",
        "            incoming_data = await websocket.recv()\n",
        "            incoming_data = json.loads(incoming_data)\n",
        "\n",
        "            match incoming_data['event']:\n",
        "                case 'text_stream':\n",
        "                    yield incoming_data['text']\n",
        "                case 'stream_end':\n",
        "                    return \n",
        "\n",
        "response_apistream =\"\"\n",
        "\n",
        "async def print_response_stream(prompt, temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "    async for response in run(prompt, temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "        global response_apistream\n",
        "        response_apistream = response_apistream + response\n",
        "\n",
        "def stop_stream():\n",
        "    stop_url = f'http://{HOST}/api/v1/stop'\n",
        "    response = requests.post(stop_url)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Stream stopped successfully.\")\n",
        "    else:\n",
        "        print(\"Failed to stop the stream.\")        \n",
        "\n",
        "is_stream_running = False\n",
        "\n",
        "def api_stream(temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "    global question_text, is_stream_running\n",
        "    asyncio.run(print_response_stream(question_text, temperature, top_p, typical_p, repetition_penalty, top_k))\n",
        "    #is_stream_running = False\n",
        "\n",
        "\n",
        "def start_api_stream(temperature, top_p, typical_p, repetition_penalty, top_k):\n",
        "   global is_stream_running\n",
        "   if not is_stream_running:\n",
        "        t1 = threading.Thread(target=api_stream, args=(temperature, top_p, typical_p, repetition_penalty, top_k))\n",
        "        t1.start()\n",
        "        is_stream_running = True\n",
        "\n",
        "#example prompt  \n",
        "question_text =\"This is a conversation with your Assistant. The Assistant is very helpful and is eager to chat with you and answer your questions. You: 4 + 102= ? Assistant:\"\n",
        "answer_text = \"\"\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def index():\n",
        "\n",
        "    if request.method == 'POST':\n",
        "        button_clicked = 'button_status' in request.form\n",
        "        stream_enabled = 'stream_enable' in request.form\n",
        "        temperature = float(request.form.get('temperature', session.get('temperature', '0.7')))\n",
        "        top_p = float(request.form.get('top_p', session.get('top_p', '0.1')))\n",
        "        typical_p = float(request.form.get('typical_p', session.get('typical_p', '1')))\n",
        "        repetition_penalty = float(request.form.get('repetition_penalty', session.get('repetition_penalty', '1.18')))\n",
        "        top_k = int(request.form.get('top_k', session.get('top_k', '40')))\n",
        "\n",
        "        global question_text, answer_text, response_apistream, is_stream_running\n",
        "        question_text = request.form.get('prompt', '')\n",
        "\n",
        "        if button_clicked and not stream_enabled:\n",
        "            answer_text = generate(question_text, temperature, top_p, typical_p, repetition_penalty, top_k)\n",
        "\n",
        "        if response_apistream == answer_text and response_apistream != \"\":\n",
        "            button_clicked = False  # Stream end\n",
        "            is_stream_running = False\n",
        "            response_apistream = \"\"\n",
        "\n",
        "        if button_clicked and stream_enabled and not is_stream_running:\n",
        "            start_api_stream(temperature, top_p, typical_p, repetition_penalty, top_k)  # Start stream\n",
        "\n",
        "        #if button_clicked and not stream_enabled and is_stream_running:\n",
        "          # stop_stream()\n",
        "          # button_clicked = False  # Stream end\n",
        "           # is_stream_running = False\n",
        "           # response_apistream = \"\"\n",
        "        \n",
        "        if button_clicked and stream_enabled:\n",
        "            \n",
        "            if response_apistream == \"\":\n",
        "                answer_text = \"Generating...\"\n",
        "            else:\n",
        "                answer_text = response_apistream  # Copy stream chunk\n",
        "\n",
        "        log_text = log_line()\n",
        "\n",
        "        session['temperature'] = temperature\n",
        "        session['top_p'] = top_p\n",
        "        session['typical_p'] = typical_p\n",
        "        session['repetition_penalty'] = repetition_penalty\n",
        "        session['top_k'] = top_k\n",
        "\n",
        "    else:\n",
        "        #question_text = request.form.get('prompt', '')\n",
        "        answer_text = ''\n",
        "        log_text = ''\n",
        "        stream_enabled = True\n",
        "        button_clicked = False\n",
        "        temperature = float(session.get('temperature', '0.7'))\n",
        "        top_p = float(session.get('top_p', '0.1'))\n",
        "        typical_p = float(session.get('typical_p', '1'))\n",
        "        repetition_penalty = float(session.get('repetition_penalty', '1.18'))\n",
        "        top_k = int(session.get('top_k', '40'))\n",
        "\n",
        "    return render_template_string('''\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Simple Colab UI</title>\n",
        "    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
        "    <script>\n",
        "        function updateSliderValue(slider) {\n",
        "            var sliderId = slider.id;\n",
        "            var valueElement = document.getElementById(sliderId + 'Value');\n",
        "            valueElement.innerText = slider.value;\n",
        "            }\n",
        "\n",
        "        function simulateButtonClick() {         //Workaround to get data from colab web url, due o lack of suport of data requests like json etc.        \n",
        "           if ($(\"#stream_enable\").is(\":checked\")) {\n",
        "                document.forms[0].submit();  // Send POST in loop\n",
        "            }\n",
        "        }\n",
        "\n",
        "        function startTimer() {\n",
        "            setInterval(simulateButtonClick, 2500); // 2.5 seconds\n",
        "                 // document.forms[0].submit();  // Send POST first time after enabling stream\n",
        "        }\n",
        "\n",
        "        $(document).ready(function(){\n",
        "\n",
        "            $(\"#loader\").hide();   \n",
        "\n",
        "            $(\"form\").submit(function(event){  // Send Text button function definition \n",
        "\n",
        "                $(\"#button_status\").prop(\"checked\", true); // button click    \n",
        "              //      event.preventDefault(); // Disable sending POST\n",
        "\n",
        "                $(\"#loader\").show();\n",
        "                startTime = new Date();\n",
        "                $(\"#answer\").val(\"Generating...  0s\");\n",
        "                timer = setInterval(function() {\n",
        "                    var currentTime = new Date();\n",
        "                    var elapsedTime = Math.round((currentTime - startTime) / 1000);\n",
        "                    $(\"#answer\").val(\"Generating...  \" + elapsedTime + \"s\");                   \n",
        "                }, 1000);\n",
        "  \n",
        "            });\n",
        "\n",
        "            if ($(\"#stream_enable\").is(\":checked\") && $(\"#button_status\").is(\":checked\")) {\n",
        "                 startTimer(); // run cyclic request for streamdata \n",
        "                   } \n",
        "        });\n",
        "    </script>\n",
        "    <style>\n",
        "        #loader {\n",
        "            border: 5px solid #f3f3f3;\n",
        "            border-top: 5px solid #3498db;\n",
        "            border-right: 5px solid #3498db;\n",
        "            border-bottom: 5px solid #f3f3f3;\n",
        "            border-left: 5px solid #f3f3f3;\n",
        "            border-radius: 50%;\n",
        "            width: 10px;\n",
        "            height: 10px;\n",
        "            animation: spin 1.5s linear infinite;\n",
        "        }\n",
        "\n",
        "        @keyframes spin {\n",
        "            0% { transform: rotate(0deg); }\n",
        "            100% { transform: rotate(360deg); }\n",
        "        }\n",
        "\n",
        "        .hidden-checkbox {\n",
        "        position: absolute;\n",
        "        left: -9999px;\n",
        "    }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <form method=\"POST\">\n",
        "           <label for=\"question\">Question:</label><br>\n",
        "        <textarea id=\"question\" name=\"prompt\" cols=\"160\" rows=\"3\">{{ question_text }}</textarea><br>\n",
        "        <label for=\"answer\">Answer:</label><br>\n",
        "        <textarea id=\"answer\" name=\"answer\" cols=\"160\" rows=\"16\">{{ answer_text }}</textarea><br>\n",
        "\n",
        "        <input type=\"submit\" id=\"sendTextButton\" value=\"Send Text\" \"><br><br>\n",
        "        <input type=\"checkbox\" id=\"stream_enable\" name=\"stream_enable\" {% if stream_enabled %}checked{% endif %}>\n",
        "        <label for=\"stream_enable\">Enable Streaming</label><br><br>&nbsp&nbsp\n",
        "\n",
        "        <label for=\"temperature\">Temperature:</label>\n",
        "        <input type=\"range\" id=\"temperature\" name=\"temperature\" min=\"0\" max=\"2\" step=\"0.01\" value=\"{{ temperature }}\" oninput=\"updateSliderValue(this)\">\n",
        "        <span id=\"temperatureValue\">{{ temperature }}</span>&nbsp&nbsp\n",
        "        <label for=\"top_p\">Top P:</label>\n",
        "        <input type=\"range\" id=\"top_p\" name=\"top_p\" min=\"0\" max=\"1\" step=\"0.01\" value=\"{{ top_p }}\" oninput=\"updateSliderValue(this)\">\n",
        "        <span id=\"top_pValue\">{{ top_p }}</span>&nbsp&nbsp\n",
        "        <label for=\"typical_p\">Typical P:</label>\n",
        "        <input type=\"range\" id=\"typical_p\" name=\"typical_p\" min=\"0\" max=\"1\" step=\"0.01\" value=\"{{ typical_p }}\" oninput=\"updateSliderValue(this)\">\n",
        "        <span id=\"typical_pValue\">{{ typical_p }}</span>&nbsp&nbsp\n",
        "        <label for=\"repetition_penalty\">Repetition Penalty:</label>\n",
        "        <input type=\"range\" id=\"repetition_penalty\" name=\"repetition_penalty\" min=\"0\" max=\"1.5\" step=\"0.01\" value=\"{{ repetition_penalty }}\" oninput=\"updateSliderValue(this)\">\n",
        "        <span id=\"repetition_penaltyValue\">{{ repetition_penalty }}</span>&nbsp&nbsp\n",
        "        <label for=\"top_k\">Top K:</label>\n",
        "        <input type=\"range\" id=\"top_k\" name=\"top_k\" min=\"1\" max=\"200\" step=\"1\" value=\"{{ top_k }}\" oninput=\"updateSliderValue(this)\">\n",
        "        <span id=\"top_kValue\">{{ top_k }}</span>&nbsp&nbsp\n",
        "        <input type=\"checkbox\" id=\"button_status\" name=\"button_status\" class=\"hidden-checkbox\" {% if button_clicked %}checked{% endif %}> <br>   \n",
        "        <p>{{ log_text }}</p>    \n",
        "        <div id=\"loader\"></div>\n",
        "    </form>\n",
        "</body>\n",
        "</html>\n",
        "    ''', question_text=question_text, answer_text=answer_text, log_text=log_text, stream_enabled=stream_enabled, button_clicked=button_clicked, temperature=temperature, top_p=top_p, typical_p=typical_p, repetition_penalty=repetition_penalty, top_k=top_k)\n",
        "\n",
        "socketio.run(app, port=iport)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA3wh0Oc18Fa"
      },
      "outputs": [],
      "source": [
        "#@title  Close main server\n",
        "#Close main server\n",
        "!pkill -f -e -c server.py # stop server\n",
        "!fuser -k 5001/tcp  # close UI port \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xkDjS3c8Pr_3"
      },
      "outputs": [],
      "source": [
        "#@title optional install \"LLaMa\" character file i found on reddit\n",
        "!wget https://github.com/pcrii/Philo-Colab-Collection/raw/main/llama.json \n",
        "!mv llama.json {repo_dir}/characters\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}